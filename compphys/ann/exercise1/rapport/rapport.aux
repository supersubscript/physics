\relax 
\bibstyle{natbib}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Problems}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}XOR logic}{2}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Results when trying to solve the XOR logic with a neural net constructed by two hidden nodes. Every method experienced 1000 simulations.\relax }}{2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:xor}{{1}{2}}
\newlabel{tab:xor@cref}{{[table][1][]1}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Synthetic Data}{3}}
\newlabel{fig:6}{{\caption@xref {fig:6}{ on input line 162}}{3}}
\newlabel{fig:6@cref}{{[subsection][2][2]2.2}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Graphical results of the training performance for the synthetic data set. The linear function shows the decision boundary produced by the neural network.\relax }}{3}}
\newlabel{fig:8_data}{{\caption@xref {fig:8_data}{ on input line 176}}{4}}
\newlabel{fig:8_data@cref}{{[subsection][2][2]2.2}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Depiction of the problematic in trying to discern the two classes from each other. The graph itself shows the validation performance of a 15 node network.\relax }}{4}}
\newlabel{fig:8_12_nodes}{{\caption@xref {fig:8_12_nodes}{ on input line 200}}{5}}
\newlabel{fig:8_12_nodes@cref}{{[subsection][2][2]2.2}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Classification output for synthetic data set 1, with 12 hidden nodes.\relax }}{5}}
\newlabel{fig:8_100_nodes}{{\caption@xref {fig:8_100_nodes}{ on input line 208}}{5}}
\newlabel{fig:8_100_nodes@cref}{{[subsection][2][2]2.2}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Classification output for synthetic data set 1, with 100 hidden nodes. Note that there are vague signs of overtraining relative to fig.~3.\relax }}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Training versus validation performance (fraction of correct classifications) on synthetic data set 1. Note that the validation performance is consistently lower than the training one.\relax }}{5}}
\newlabel{fig:7_and_8_comp}{{5}{5}}
\newlabel{fig:7_and_8_comp@cref}{{[figure][5][]5}{5}}
\newlabel{fig:9_data}{{\caption@xref {fig:9_data}{ on input line 241}}{6}}
\newlabel{fig:9_data@cref}{{[subsection][2][2]2.2}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Training sample of synthetic data set 3. Graph shows training classification for a 15 node network.\relax }}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Validation performance for different sizes of a single-layer MLP on synthetic data set 3.\relax }}{7}}
\newlabel{fig:9}{{7}{7}}
\newlabel{fig:9@cref}{{[figure][7][]7}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Liver Data}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Time Series}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Performance for single-layer network with 7 hidden nodes and the LM algorithm on the sunspot data.\relax }}{8}}
\newlabel{fig:11}{{8}{8}}
\newlabel{fig:11@cref}{{[figure][8][]8}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Mackey-Glass}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Performance for single-layer network with 21 hidden nodes and the LM algorithm on the Mackey-Glass data.\relax }}{9}}
\newlabel{fig:12}{{9}{9}}
\newlabel{fig:12@cref}{{[figure][9][]9}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Challenge Exercise}{9}}
\bibcite{lecnotes}{1}
\bibcite{excnotes}{2}
\@writefile{toc}{\contentsline {section}{\numberline {3}Concluding remarks}{10}}
